# LLM Configuration
# Copy this file to .env and fill in your API key

# API Key (required for real LLM, leave empty for Mock)
LLM_API_KEY=

# API Base URL (supports OpenAI, Azure, Claude proxy, Ollama, or your gateway)
# Default: https://api.openai.com/v1
# Examples:
#   OpenAI:     https://api.openai.com/v1
#   Azure:      https://YOUR_RESOURCE.openai.azure.com
#   Ollama:     http://localhost:11434/v1
#   Gateway:    http://localhost:8080/v1
LLM_BASE_URL=https://api.openai.com/v1

# Model name
# Default: gpt-4o-mini
# Examples:
#   OpenAI:     gpt-4o, gpt-4o-mini, gpt-4-turbo
#   Claude:     claude-3-opus, claude-3-sonnet
#   Ollama:     llama3, qwen2, mistral
LLM_MODEL=gpt-4o-mini

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Context Compression Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# AUTO_COMPRESS_THRESHOLD: Number of messages before auto-compression triggers
# Set to 0 to disable auto-compression (manual /compress only)
# Default: 50
# Examples:
#   GPT-4 (128K):     100
#   DeepSeek (64K):   50
#   Llama3 (8K):      20
AUTO_COMPRESS_THRESHOLD=20

# COMPRESS_KEEP_TURNS: Number of recent turns to keep after compression
# Default: 3
# A "turn" = one user message + following assistant/tool responses
COMPRESS_KEEP_TURNS=3

FILTER_HISTORY_TOOLS=true
